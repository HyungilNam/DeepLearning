{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "경사법 예시\n",
      "[ 0.  4.]\n",
      "[ 0.  4.]\n",
      "[ 6.  0.]\n"
     ]
    }
   ],
   "source": [
    "print('경사법 예시')\n",
    "import numpy as np\n",
    "\n",
    "def function_2(x):\n",
    "    return np.sum(x**2)\n",
    "\n",
    "def numerical_gradient(f,x):\n",
    "    h = 1e-4 #0.0001\n",
    "    grad = np.zeros_like(x)  #x와 형상이 같은 배열을 생성\n",
    "    \n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "        \n",
    "        #f(x + h)계산\n",
    "        x[idx] = tmp_val +h\n",
    "        fxh1 = f(x)\n",
    "        \n",
    "        #f(x - h)계산\n",
    "        x[idx] = tmp_val -h\n",
    "        fxh2 = f(x)  \n",
    "        \n",
    "        grad[idx] = (fxh1 - fxh2)/(2*h)\n",
    "        x[idx] = tmp_val  #값 복원\n",
    "        \n",
    "    return grad\n",
    "\n",
    "\n",
    "print(numerical_gradient(function_2, np.array([0.0, 2.0])))\n",
    "print(numerical_gradient(function_2, np.array([0.0, 2.0])))\n",
    "print(numerical_gradient(function_2, np.array([3.0, 0.0])))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "경사 하강법 예시\n",
      "-3.0, 4.0에서 경사법으로 최소값 탐색\n",
      "[ -6.11110793e-10   8.14814391e-10]\n",
      "학습률이 너무 큰 예: lr =10.0인 경우\n",
      "[  2.34235971e+12  -3.96091057e+12]\n",
      "학습률이 너무 작은 예: lr =1e-10인 경우\n",
      "[  2.34235971e+12  -3.96091057e+12]\n"
     ]
    }
   ],
   "source": [
    "print('경사 하강법 예시')\n",
    "\n",
    "def gradient_descent(f, init_x, lr =0.01, step_num=100):\n",
    "    x = init_x\n",
    "    \n",
    "    for i in range(step_num):\n",
    "        grad = numerical_gradient(f, x)\n",
    "        x -= lr*grad\n",
    "        \n",
    "    return x\n",
    "\n",
    "print('-3.0, 4.0에서 경사법으로 최소값 탐색')\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "print(gradient_descent(function_2, init_x=init_x, lr =0.1, step_num=100))\n",
    "\n",
    "print('학습률이 너무 큰 예: lr =10.0인 경우')\n",
    "print(gradient_descent(function_2, init_x=init_x, lr =10.0, step_num=100))\n",
    "\n",
    "print('학습률이 너무 작은 예: lr =1e-10인 경우')\n",
    "print(gradient_descent(function_2, init_x=init_x, lr =1e-10, step_num=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "신경망 기울기 예시\n",
      "[[ 1.77111385  0.96331216 -0.74215061]\n",
      " [ 0.18331289  0.59383651 -1.56567128]]\n",
      "\n",
      "\n",
      "[ 1.22764991  1.11244016 -1.85439452]\n",
      "\n",
      "\n",
      "3.74320360953\n",
      "\n",
      "\n",
      "[[ 0.30974892  0.27604179 -0.58579071]\n",
      " [ 0.46462338  0.41406269 -0.87868607]]\n"
     ]
    }
   ],
   "source": [
    "print('신경망 기울기 예시')\n",
    "import numpy as np\n",
    "\n",
    "#softmaxt function, cross_entropy_error function, numerical_gradient function\n",
    "def softmax(a):\n",
    "    c = np.max(a)\n",
    "    exp_a = np.exp(a-c) #오버플로 대책\n",
    "    sum_exp_a = np.sum(exp_a)\n",
    "    y = exp_a/sum_exp_a\n",
    "    \n",
    "    return y\n",
    "\n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "\n",
    "    # 훈련 데이터가 원-핫 벡터라면 정답 레이블의 인덱스로 반환\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "\n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n",
    "\n",
    "def numerical_gradient(f, x):  #기존의 책의 내용과 차이가 있음! 주의!\n",
    "    h = 1e-4 # 0.0001\n",
    "    grad = np.zeros_like(x)\n",
    "\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        idx = it.multi_index\n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = float(tmp_val) + h\n",
    "        fxh1 = f(x) # f(x+h)\n",
    "\n",
    "        x[idx] = tmp_val - h\n",
    "        fxh2 = f(x) # f(x-h)\n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "\n",
    "        x[idx] = tmp_val # 값 복원\n",
    "        it.iternext()\n",
    "\n",
    "    return grad\n",
    "\n",
    "\n",
    "#기울기를 구하기 위한 환경 class설정\n",
    "class simpleNet:\n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(2,3) #정규분포로 초기화\n",
    "        \n",
    "    def predict(self,x):\n",
    "        return np.dot(x, self.W)\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        z= self.predict(x)\n",
    "        y= softmax(z)\n",
    "        loss = cross_entropy_error(y, t)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    \n",
    "net = simpleNet()\n",
    "print(net.W)  #가중치 매개변수\n",
    "print('\\n')\n",
    "\n",
    "x = np.array([0.6, 0.9])\n",
    "p = net.predict(x)\n",
    "print(p)\n",
    "print('\\n')\n",
    "np.argmax(p)  #최대값의 인덱스\n",
    "\n",
    "t = np.array([0, 0,1])  #정답 레이블\n",
    "print(net.loss(x,t))\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "# 기울기 구하기\n",
    "def f(W):\n",
    "    return net.loss(x, t)\n",
    "\n",
    "dW = numerical_gradient(f, net.W)\n",
    "print(dW)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
